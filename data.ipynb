{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole dataset size 23209\n",
      "Number of train images\n",
      "34\n",
      "Number of  train images  34\n",
      "whole dataset size 23209\n",
      "Number of val images\n",
      "7\n",
      "Number of  val images  7\n",
      "0 3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 11.62 GiB of which 1.08 GiB is free. Process 3154473 has 7.06 GiB memory in use. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 18.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m img, crops, labels \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcuda(), crops\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 198\u001b[0m     pos, neg, b_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     loss \u001b[38;5;241m=\u001b[39m pos \u001b[38;5;241m+\u001b[39m neg \u001b[38;5;241m+\u001b[39m b_loss\n\u001b[1;32m    200\u001b[0m     loss_train\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/repos/VoCoPretrain/models/voco_head.py:183\u001b[0m, in \u001b[0;36mVoCoHead.forward\u001b[0;34m(self, img, crops, labels)\u001b[0m\n\u001b[1;32m    180\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([img, crops], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# here we do norm on all instances\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m students_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_EMA_update_encoder_teacher()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/repos/VoCoPretrain/models/voco_head.py:142\u001b[0m, in \u001b[0;36mSwin.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_in):\n\u001b[1;32m    141\u001b[0m     b \u001b[38;5;241m=\u001b[39m x_in\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 142\u001b[0m     hidden_states_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswinViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     enc0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder1(x_in)\n\u001b[1;32m    145\u001b[0m     enc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2(hidden_states_out[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:1064\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x, normalize)\u001b[0m\n\u001b[1;32m   1062\u001b[0m x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m   1063\u001b[0m x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x0)\n\u001b[0;32m-> 1064\u001b[0m x0_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_v2:\n\u001b[1;32m   1066\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers1c[\u001b[38;5;241m0\u001b[39m](x0\u001b[38;5;241m.\u001b[39mcontiguous())\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/monai/networks/nets/swin_unetr.py:1052\u001b[0m, in \u001b[0;36mSwinTransformer.proj_out\u001b[0;34m(self, x, normalize)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     n, ch, d, h, w \u001b[38;5;241m=\u001b[39m x_shape\n\u001b[1;32m   1051\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn c d h w -> n d h w c\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1052\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn d h w c -> n c d h w\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x_shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 11.62 GiB of which 1.08 GiB is free. Process 3154473 has 7.06 GiB memory in use. Including non-PyTorch memory, this process has 3.42 GiB memory in use. Of the allocated memory 3.18 GiB is allocated by PyTorch, and 18.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from time import time\n",
    "from datasets.BrainImagingDataset_3D import BrainImgDataset\n",
    "from  monai import transforms\n",
    "import numpy as np\n",
    "from collections.abc import Callable, Sequence\n",
    "from torch.utils.data import Dataset as _TorchDataset\n",
    "from torch.utils.data import Subset\n",
    "import collections\n",
    "import numpy as np\n",
    "from monai.data import *\n",
    "from monai.transforms import *\n",
    "import argparse\n",
    "import pickle\n",
    "from utils.brain_data_utils import threshold\n",
    "\n",
    "import torch.optim as optim\n",
    "from models.voco_head import VoCoHead\n",
    "from optimizers.lr_scheduler import WarmupCosineSchedule\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from utils.data_utils import get_position_label, get_crop_transform, get_vanilla_transform, VoCoAugmentation\n",
    "from utils.utils import AverageMeter\n",
    "\n",
    "import torch as torch\n",
    "# from lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "# from lightning.loggers import WandbLogger\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "# from lightning.callbacks import ModelCheckpoint, TQDMProgressBar, EarlyStopping,  LearningRateMonitor\n",
    "\n",
    "# from lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import monai.transforms as transforms\n",
    "from monai import data\n",
    "import argparse\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from models.voco_head import VoCoHead\n",
    "from optimizers.lr_scheduler import WarmupCosineSchedule\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.data_utils import *\n",
    "from utils.ops import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Swin UNETR segmentation pipeline\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "args.roi = 142\n",
    "args.num_workers = 1\n",
    "args.logdir = \"logs\"\n",
    "args.epochs = 10\n",
    "args.num_steps = 100  #\n",
    "args.eval_num = 100\n",
    "args.warmup_steps = 5000\n",
    "args.in_channels = 1\n",
    "args.feature_size = 48 #\n",
    "\n",
    "args.dropout_path_rate = 0.0\n",
    "args.use_checkpoint = True\n",
    "args.spatial_dims = 3\n",
    "args.a_min = -175.0\n",
    "args.a_max = 250.0\n",
    "args.b_min = 0.0\n",
    "args.b_max = 1.0\n",
    "args.space_x = 1.5\n",
    "args.space_y = 1.5\n",
    "args.space_z = 1.5\n",
    "args.roi_x = roi\n",
    "args.roi_y = roi\n",
    "args.roi_z = roi\n",
    "args.batch_size = 1\n",
    "args.sw_batch_size = 2\n",
    "args.lr = 1e-4\n",
    "args.decay = 0.1\n",
    "args.momentum = 0.9\n",
    "args.lrdecay = True\n",
    "args.max_grad_norm = 1.0\n",
    "args.loss_type = \"SSL\"\n",
    "args.opt = \"adamw\"\n",
    "args.lr_schedule = \"warmup_cosine\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paths  = {    'root_dir':  '/data2/biodata/biobank/brain_data/',\n",
    "    'table_dir' : '/data2/biodata/biobank/tables/'}\n",
    "root_dir =  paths['root_dir']\n",
    "modality_path = 'T1/T1_brain_to_MNI.nii.gz'\n",
    "table_dir = paths['table_dir']\n",
    "\n",
    "split = 0.002\n",
    "batch_size=1\n",
    "num_workers=4\n",
    "\n",
    "datalist = BrainImgDataset(root_dir, modality_path,table_dir, percent_split = split,split='train').dict_paths\n",
    "val_files = BrainImgDataset(root_dir, modality_path,table_dir,percent_split = split, split='val').dict_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([LoadImaged(keys=[\"image\"], image_only=True, dtype=np.int16),\n",
    "                                EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                                Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "                                ScaleIntensityRanged(\n",
    "                                    keys=[\"image\"], a_min=args.a_min, a_max=args.a_max,\n",
    "                                    b_min=args.b_min, b_max=args.b_max, clip=True),\n",
    "                                SpatialPadd(keys=\"image\", spatial_size=[args.roi_x, args.roi_y,\n",
    "                                                                        args.roi_z]),\n",
    "                                CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "                                SpatialCropd(keys=[\"image\"], roi_start=[60, 80, 0],\n",
    "                                             roi_end=[440, 380, 10000]),\n",
    "                                Resized(keys=[\"image\"], mode=\"trilinear\", align_corners=True,\n",
    "                                        spatial_size=(384, 384, 96)),\n",
    "\n",
    "                                # Random\n",
    "                                RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.0),\n",
    "                                CropForegroundd(keys=\"image\", source_key=\"image\", select_fn=threshold),\n",
    "                                Resized(keys=\"image\", mode=\"bilinear\", align_corners=True,\n",
    "                                        spatial_size=(384, 384, 96)),\n",
    "\n",
    "                                VoCoAugmentation(args, aug=True)\n",
    "                                ])\n",
    "\n",
    "val_transforms = transforms.Compose([LoadImaged(keys=[\"image\"], image_only=True, dtype=np.int16),\n",
    "                              EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                              Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "                              ScaleIntensityRanged(\n",
    "                                  keys=[\"image\"], a_min=args.a_min, a_max=args.a_max,\n",
    "                                  b_min=args.b_min, b_max=args.b_max, clip=True),\n",
    "                              SpatialPadd(keys=\"image\", spatial_size=[args.roi_x, args.roi_y,\n",
    "                                                                      args.roi_z]),\n",
    "                              CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "                              SpatialCropd(keys=[\"image\"], roi_start=[60, 80, 0],\n",
    "                                           roi_end=[440, 380, 10000]),\n",
    "                              Resized(keys=[\"image\"], mode=\"trilinear\", align_corners=True,\n",
    "                                      spatial_size=(384, 384, 96)),\n",
    "                              VoCoAugmentation(args, aug=False)\n",
    "                              ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds = data.PersistentDataset(data=datalist,\n",
    "                                     transform=train_transforms,\n",
    "                                     pickle_protocol=pickle.HIGHEST_PROTOCOL,\n",
    "                                     cache_dir='/data/saved/voco_cache')\n",
    "train_sampler = None\n",
    "train_loader = data.DataLoader(\n",
    "        train_ds, batch_size=batch_size, num_workers=num_workers, sampler=None, shuffle=True,\n",
    "        drop_last=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "model = VoCoHead(args)\n",
    "\n",
    "model.train()\n",
    "loss_train = []\n",
    "run_loss = AverageMeter()\n",
    "pos_avg, neg_avg, base_avg = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "torch.cuda.set_device(0)\n",
    "model.cuda()\n",
    "\n",
    "def concat_image(imgs):\n",
    "    output = []\n",
    "    for img in imgs:\n",
    "        img = img['image']\n",
    "        output.append(img)\n",
    "        \n",
    "    output = torch.concatenate(output, dim=1)\n",
    "\n",
    "    bs, sw_s, x, y, z = output.size()\n",
    "    output = output.view(-1, 1, x, y, z)\n",
    "    return output\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "    t1 = time()\n",
    "    img, labels, crops = batch\n",
    "    print(step, len(batch))\n",
    "\n",
    "    img, crops = concat_image(img), concat_image(crops)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(img.size(), crops.size(), labels.size())\n",
    "    img, crops, labels = img.cuda(), crops.cuda(), labels.cuda()\n",
    "\n",
    "    with autocast(enabled=False):\n",
    "        pos, neg, b_loss = model(img, crops, labels)\n",
    "        loss = pos + neg + b_loss\n",
    "        loss_train.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from datasets.BrainImagingDataset_3D import BrainImgDataset\n",
    "from  monai import transforms\n",
    "import numpy as np\n",
    "from collections.abc import Callable, Sequence\n",
    "from torch.utils.data import Dataset as _TorchDataset\n",
    "from torch.utils.data import Subset\n",
    "import collections\n",
    "import numpy as np\n",
    "from monai import data\n",
    "import argparse\n",
    "import pickle\n",
    "from utils.brain_data_utils import threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole dataset size 23209\n",
      "Number of train images\n",
      "3480\n",
      "Number of  train images  3480\n",
      "whole dataset size 23209\n",
      "Number of val images\n",
      "696\n",
      "Number of  val images  696\n"
     ]
    }
   ],
   "source": [
    "paths  = {    'root_dir':  '/data2/biodata/biobank/brain_data/',\n",
    "    'table_dir' : '/data2/biodata/biobank/tables/'}\n",
    "root_dir =  paths['root_dir']\n",
    "modality_path = 'T1/T1_brain_to_MNI.nii.gz'\n",
    "table_dir = paths['table_dir']\n",
    "\n",
    "split = 0.2\n",
    "\n",
    "datalist = BrainImgDataset(root_dir, modality_path,table_dir, percent_split = split,split='train').dict_paths\n",
    "val_files = BrainImgDataset(root_dir, modality_path,table_dir,percent_split = split, split='val').dict_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = 64\n",
    "num_workers = 1\n",
    "logdir = \"logs\"\n",
    "epochs = 100\n",
    "num_steps = 250000\n",
    "eval_num = 100\n",
    "warmup_steps = 5000\n",
    "in_channels = 1\n",
    "feature_size = 48\n",
    "dropout_path_rate = 0.0\n",
    "use_checkpoint = True\n",
    "spatial_dims = 3\n",
    "a_min = -175.0\n",
    "a_max = 250.0\n",
    "b_min = 0.0\n",
    "b_max = 1.0\n",
    "space_x = 1.5\n",
    "space_y = 1.5\n",
    "space_z = 1.5\n",
    "roi_x = roi\n",
    "roi_y = roi\n",
    "roi_z = roi\n",
    "batch_size = 1\n",
    "sw_batch_size = 2\n",
    "lr = 1e-4\n",
    "decay = 0.1\n",
    "momentum = 0.9\n",
    "lrdecay = True\n",
    "max_grad_norm = 1.0\n",
    "loss_type = \"SSL\"\n",
    "opt = \"adamw\"\n",
    "lr_schedule = \"warmup_cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 69 [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.16276042 0.04557292 0.         0.\n",
      "  0.61848958 0.17317708 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class VoCoAugmentation():\n",
    "    def __init__(self,  aug):\n",
    "\n",
    "        self.aug = aug\n",
    "\n",
    "    def __call__(self, x_in):\n",
    "        crops_trans = get_crop_transform(roi_small=roi_x, aug=self.aug)\n",
    "\n",
    "        vanilla_trans, labels = get_vanilla_transform(num=sw_batch_size,\n",
    "                                                      roi_small=roi_x, aug=self.aug)\n",
    "\n",
    "        imgs = []\n",
    "        for trans in vanilla_trans:\n",
    "            img = trans(x_in)\n",
    "            imgs.append(img)\n",
    "\n",
    "        crops = []\n",
    "        for trans in crops_trans:\n",
    "            crop = trans(x_in)\n",
    "            crops.append(crop)\n",
    "\n",
    "        return imgs, labels, crops\n",
    "\n",
    "\n",
    "def get_vanilla_transform(num=2, num_crops=4, roi_small=64, roi=96, max_roi=384, aug=False):\n",
    "    vanilla_trans = []\n",
    "    labels = []\n",
    "    for i in range(num):\n",
    "        center_x, center_y, label = get_position_label(roi=roi,\n",
    "                                                       max_roi=max_roi,\n",
    "                                                       num_crops=num_crops)\n",
    "        if aug:\n",
    "            trans = transforms.Compose([\n",
    "               transforms.SpatialCropd(keys=['image'],\n",
    "                             roi_center=[center_x, center_y, roi // 2],\n",
    "                             roi_size=[roi, roi, roi]),\n",
    "               transforms.Resized(keys=[\"image\"], mode=\"bilinear\", align_corners=True,\n",
    "                        spatial_size=(roi_small, roi_small, roi_small)),\n",
    "               transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=0),\n",
    "               transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=1),\n",
    "               transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=2),\n",
    "                transforms.RandRotate90d(keys=[\"image\"], prob=0.2, max_k=3),\n",
    "                transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.1),\n",
    "                transforms.ToTensord(keys=[\"image\"])])\n",
    "        else:\n",
    "            trans = transforms.Compose([\n",
    "                transforms.SpatialCropd(keys=['image'],\n",
    "                             roi_center=[center_x, center_y, roi // 2],\n",
    "                             roi_size=[roi, roi, roi]),\n",
    "               transforms.Resized(keys=[\"image\"], mode=\"bilinear\", align_corners=True,\n",
    "                        spatial_size=(roi_small, roi_small, roi_small)),\n",
    "                transforms.ToTensord(keys=[\"image\"])])\n",
    "\n",
    "        vanilla_trans.append(trans)\n",
    "        labels.append(label)\n",
    "\n",
    "    labels = np.concatenate(labels, 0).reshape(num, num_crops * num_crops)\n",
    "\n",
    "\n",
    "    return vanilla_trans, labels\n",
    "\n",
    "\n",
    "def get_crop_transform(num=4, roi_small=64, roi=96, aug=False):\n",
    "    voco_trans = []\n",
    "    # not symmetric at axis x !!!\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            center_x = (i + 1 / 2) * roi\n",
    "            center_y = (j + 1 / 2) * roi\n",
    "            center_z = roi // 2\n",
    "\n",
    "            if aug:\n",
    "                trans = transforms.Compose([\n",
    "                    transforms.SpatialCropd(keys=['image'],\n",
    "                                 roi_center=[center_x, center_y, center_z],\n",
    "                                 roi_size=[roi, roi, roi]),\n",
    "                   transforms.Resized(keys=[\"image\"],\n",
    "                            mode=\"bilinear\",\n",
    "                            align_corners=True,\n",
    "                            spatial_size=(roi_small, roi_small, roi_small)\n",
    "                            ),\n",
    "                   transforms.Resized(keys=[\"image\"], mode=\"bilinear\", align_corners=True,\n",
    "                            spatial_size=(roi_small, roi_small, roi_small)),\n",
    "                    transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=0),\n",
    "                    transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=1),\n",
    "                    transforms.RandFlipd(keys=[\"image\"], prob=0.2, spatial_axis=2),\n",
    "                    transforms.RandRotate90d(keys=[\"image\"], prob=0.2, max_k=3),\n",
    "                    transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.1),\n",
    "                    transforms.ToTensord(keys=[\"image\"])],\n",
    "                )\n",
    "            else:\n",
    "                trans = transforms.Compose([\n",
    "                    transforms.SpatialCropd(keys=['image'],\n",
    "                                 roi_center=[center_x, center_y, center_z],\n",
    "                                 roi_size=[roi, roi, roi]),\n",
    "                   transforms.Resized(keys=[\"image\"],\n",
    "                            mode=\"bilinear\",\n",
    "                            align_corners=True,\n",
    "                            spatial_size=(roi_small, roi_small, roi_small)\n",
    "                            ),\n",
    "                    transforms.ToTensord(keys=[\"image\"])],\n",
    "                )\n",
    "\n",
    "            voco_trans.append(trans)\n",
    "\n",
    "    return voco_trans\n",
    "\n",
    "\n",
    "def get_position_label(roi=96, base_roi=96, max_roi=384, num_crops=4):\n",
    "    half = roi // 2\n",
    "    center_x, center_y = np.random.randint(low=half, high=max_roi - half), \\\n",
    "        np.random.randint(low=half, high=max_roi - half)\n",
    "    # center_x, center_y = np.random.randint(low=half, high=half+1), \\\n",
    "    #     np.random.randint(low=half, high=half+1)\n",
    "    # center_x, center_y = roi + half, roi + half\n",
    "    # print(center_x, center_y)\n",
    "\n",
    "    x_min, x_max = center_x - half, center_x + half\n",
    "    y_min, y_max = center_y - half, center_y + half\n",
    "\n",
    "    total_area = roi * roi\n",
    "    labels = []\n",
    "    for i in range(num_crops):\n",
    "        for j in range(num_crops):\n",
    "            crop_x_min, crop_x_max = i * base_roi, (i + 1) * base_roi\n",
    "            crop_y_min, crop_y_max = j * base_roi, (j + 1) * base_roi\n",
    "\n",
    "            dx = min(crop_x_max, x_max) - max(crop_x_min, x_min)\n",
    "            dy = min(crop_y_max, y_max) - max(crop_y_min, y_min)\n",
    "            if dx <= 0 or dy <= 0:\n",
    "                area = 0\n",
    "            else:\n",
    "                area = (dx * dy) / total_area\n",
    "            labels.append(area)\n",
    "\n",
    "    labels = np.asarray(labels).reshape(1, num_crops * num_crops)\n",
    "\n",
    "    return center_x, center_y, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    center_x, center_y, labels = get_position_label()\n",
    "    print(center_x, center_y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "transforms.Compose([transforms.LoadImaged(keys=[\"image\"], image_only=True, dtype=np.int16),\n",
    "                                transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                                transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "                                transforms.ScaleIntensityRanged(\n",
    "                                    keys=[\"image\"], a_min=a_min, a_max=a_max,\n",
    "                                    b_min=b_min, b_max=b_max, clip=True),\n",
    "                                transforms.SpatialPadd(keys=\"image\", spatial_size=[roi_x, roi_y,\n",
    "                                                                        roi_z]),\n",
    "                                transforms.CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "                                transforms.SpatialCropd(keys=[\"image\"], roi_start=[60, 80, 0],\n",
    "                                             roi_end=[440, 380, 10000]),\n",
    "                                transforms.Resized(keys=[\"image\"], mode=\"trilinear\", align_corners=True,\n",
    "                                        spatial_size=(384, 384, 96)),\n",
    "\n",
    "                                # Random\n",
    "                                transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.0),\n",
    "                                transforms.CropForegroundd(keys=\"image\", source_key=\"image\", select_fn=threshold),\n",
    "                                transforms.Resized(keys=\"image\", mode=\"bilinear\", align_corners=True,\n",
    "                                        spatial_size=(384, 384, 96)),\n",
    "\n",
    "                                VoCoAugmentation( aug=True)\n",
    "                                ])\n",
    "\n",
    "train_transforms = transforms.Compose([ transforms.LoadImaged(keys=[\"image\"], image_only=True, dtype=np.int16),\n",
    "                             transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "                             transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "                             transforms.ScaleIntensityRanged(\n",
    "                                keys=[\"image\"], a_min=a_min, a_max=a_max,\n",
    "                                b_min=b_min, b_max=b_max, clip=True),\n",
    "                             transforms.CropForegroundd(keys=\"image\", source_key=\"image\", select_fn=threshold),\n",
    "                             transforms.Resized(keys=\"image\", mode=\"bilinear\", align_corners=True,\n",
    "                                    spatial_size=(384, 384, 96)),\n",
    "                            VoCoAugmentation( aug=True)\n",
    "                            ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/voco/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/voco/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'VoCoAugmentation' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3342393) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:67\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 3342393) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      7\u001b[0m         train_ds, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, sampler\u001b[38;5;241m=\u001b[39mtrain_sampler, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#             t1 = time()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#             print(batch)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             img, labels, crops \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3342393) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_ds = data.PersistentDataset(data=datalist,\n",
    "                                     transform=train_transforms,\n",
    "                                     pickle_protocol=pickle.HIGHEST_PROTOCOL,\n",
    "                                     cache_dir='/data/saved/voco_cache')\n",
    "train_sampler = None\n",
    "train_loader = data.DataLoader(\n",
    "        train_ds, batch_size=batch_size, num_workers=num_workers, sampler=train_sampler, shuffle=True,\n",
    "        drop_last=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "#             t1 = time()\n",
    "#             print(batch)\n",
    "            img, labels, crops = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/voco/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/voco/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'VoCoAugmentation' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3373255) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:67\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 3373255) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/anaconda3/envs/voco/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3373255) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "    print(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
